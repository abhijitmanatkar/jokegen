{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04c8855-415b-4755-8081-54da72276776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 00:32:20.532429: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-30 00:32:34.633731: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 00:32:34.634203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-30 00:32:34.634220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-30 00:32:54.045157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-30 00:32:54.045212: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8be148-56c0-42d8-b079-71b2b006b800",
   "metadata": {},
   "source": [
    "### Semantic Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd2867e-c343-4f30-8cf3-13c7f70ddd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c006af-5ca8-49bc-9a78-54748815855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_weights(sentence, matcher):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    chunks = []\n",
    "\n",
    "    for match in matches:\n",
    "        match_id, start, end = match\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        # print(span.text)\n",
    "        chunks.append((start, end))\n",
    "\n",
    "    s_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "    \n",
    "    weights = []\n",
    "    for chunk in chunks:\n",
    "        \n",
    "        start, end = chunk\n",
    "        chunk_phrase = doc[start:end]\n",
    "        \n",
    "        new_sent = ' '.join([doc[:start].text, doc[end:].text])\n",
    "        # sentence.replace(chunk, '')\n",
    "        new_embedding = model.encode(new_sent, convert_to_tensor=True)\n",
    "        # print(chunk_phrase, \": \", new_sent)\n",
    "        cosine_score = util.cos_sim(s_embedding, new_embedding)\n",
    "        weights.append(((start, end), 1-cosine_score.cpu().squeeze().numpy()))\n",
    "        \n",
    "    total = sum([score for (chunk, score) in weights])\n",
    "    weights = [(chunk, score/total) for (chunk, score) in weights]\n",
    "\n",
    "    weights.sort(key = lambda x : x[1], reverse = True)\n",
    "    return doc, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbc59b-133e-434c-aac9-ee1f17690df8",
   "metadata": {},
   "source": [
    "### Chunk extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb7df49-d49b-406b-ae99-20f8bab7ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "pattern1 = [{'POS': 'ADV', 'OP': '*'}, {'POS': 'ADJ', 'OP': '*'}]\n",
    "pattern2 = [{'POS': 'PDT', 'OP': '?'}, {'POS': 'DET', 'OP': '?'},{'POS': 'ADJ', 'OP': '*'},\n",
    "           {\"POS\": {\"IN\": [\"NOUN\", \"PROPN\"]}, 'OP': '*'}]\n",
    "pattern3 = [{'POS': 'PDT', 'OP': '?'},{'POS': 'DET', 'OP': '?'},{'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'VERB'}]\n",
    "\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"pattern\",[pattern1,pattern2,pattern3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9932ffb5-ae64-4c3c-a43c-0d0553e4adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affa1d18-e94a-466d-b159-3022eb10c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did the fish say when he hit a wall? Dam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(data))\n",
    "sample = data[idx]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a170930-4c11-4d88-b8a0-83278fe6b8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 18) eating : 0.25141154590183307\n",
      "(2, 3) friend : 0.23998979890151587\n",
      "(1, 3) a friend : 0.20324643739075807\n",
      "(6, 7) problems : 0.12165270235807099\n",
      "(16, 17) tried : 0.0853775961484633\n",
      "(3, 4) tells : 0.07110016359078666\n",
      "(1, 2) a : 0.02722175570857203\n"
     ]
    }
   ],
   "source": [
    "doc, weights = get_semantic_weights(sample, matcher)\n",
    "for (start, end), weight in weights:\n",
    "    print((start,end), doc[start:end], \":\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fe4725-a8b2-4080-a1f1-e7e4170ef3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_template(sentence, matcher, masking_type='upto_tau', tau=0.4, n=0.5):\n",
    "    \n",
    "    doc, weights = get_semantic_weights(sentence, matcher)\n",
    "    \n",
    "    # Masking sentence chunks till words with upto tau cumulative weight\n",
    "    # have been masked, ignoring overlapping chunks\n",
    "    cumulative_weight = 0\n",
    "    masked = [0 for _ in range(len(doc))]\n",
    "    \n",
    "    if masking_type == 'upto_tau':\n",
    "        marker = 1\n",
    "        for (start, end), weight in weights:\n",
    "\n",
    "            if cumulative_weight + weight >= tau:\n",
    "                break\n",
    "\n",
    "            marked = False\n",
    "            for j in range(start, end):\n",
    "                if masked[j] == 0:\n",
    "                    masked[j] = marker\n",
    "                    marked = True\n",
    "\n",
    "            if marked:\n",
    "                marker += 1\n",
    "                cumulative_weight += weight\n",
    "    \n",
    "    elif masking_type == 'top_n':\n",
    "        \n",
    "        num_tokens_to_mask = n * len(doc)\n",
    "        num_masked = 0\n",
    "        \n",
    "        marker = 1\n",
    "        for (start, end), weight in weights:\n",
    "            marked = False\n",
    "            for j in range(start, end):\n",
    "                if masked[j] == 0:\n",
    "                    masked[j] = marker\n",
    "                    marked = True\n",
    "\n",
    "            if marked:\n",
    "                marker += 1\n",
    "                num_masked += end-start\n",
    "            \n",
    "            if num_masked >= num_tokens_to_mask:\n",
    "                break\n",
    "            \n",
    "    \n",
    "    # Building target template\n",
    "    sent = []\n",
    "    maskon = 0\n",
    "    for i in range(len(masked)):\n",
    "        elem = masked[i]\n",
    "        if elem == 0:\n",
    "            sent.append(doc[i])\n",
    "            maskon = 0\n",
    "        else:\n",
    "            if elem == maskon:\n",
    "                continue\n",
    "            maskon = elem\n",
    "            sent.append('[MASK]')\n",
    "            \n",
    "    return ''.join([token.text_with_ws if type(token) != str else token + ' ' for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce995fd-0f18-4b16-ae9c-8bd4f69706f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't the sun go to college? Because it had a million degrees.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(data))\n",
    "sample = data[idx]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b843de41-b5b7-4a16-89fd-c2035bd7a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why didn't [MASK] [MASK] to [MASK] ? Because it [MASK] [MASK] million [MASK] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = get_masked_template(sample, matcher, masking_type='top_n', tau=0.4, n=0.6)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd98a8c7-b92a-444f-8c67-12e5c2a199c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_data = {'Joke':[], 'Template':[]}\n",
    "# for joke in tqdm(data):\n",
    "#     template = get_masked_template(joke, matcher, tau=0.45)\n",
    "#     template_data['Joke'].append(joke)\n",
    "#     template_data['Template'].append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427f547f-dad8-4f44-8a7f-fcaa57a71d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(template_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd9b8f4-add6-42cb-b147-d84e6a776d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('test_templates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fdb427-5957-45fd-9fc7-c9badc577e23",
   "metadata": {},
   "source": [
    "### Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1922c29-cc25-4e84-8226-13adbc1c1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6dd20a4-5f77-4a1f-9e4b-72397890bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filling_model = DistilBertForMaskedLM.from_pretrained('./finetuned-distilbert')\n",
    "filling_tokenizer = DistilBertTokenizer.from_pretrained('./finetuned-distilbert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d68b75fe-b0a1-4697-ab2c-33f9348a9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filler = pipeline(\"fill-mask\", model=filling_model, tokenizer=filling_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e6b8d1-abc0-4b27-946f-365f1817d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Because you will do anything for [MASK] [MASK] of [MASK]. \n",
      " Because you will do anything for a [MASK] of [MASK]. \n",
      " Because you will do anything for a lot of [MASK]. \n",
      "Because you will do anything for a lot of money.\n"
     ]
    }
   ],
   "source": [
    "filled = template\n",
    "while '[MASK]' in filled:\n",
    "    try:\n",
    "        filled = mask_filler(filled)[0][0]['sequence']\n",
    "        filled = filled.replace('[CLS]', '')\n",
    "        filled = filled.replace('[SEP]', '')\n",
    "    except:\n",
    "        filled = mask_filler(filled)[0]['sequence']\n",
    "        filled = filled.replace('[CLS]', '')\n",
    "        filled = filled.replace('[SEP]', '')\n",
    "    print(filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503cc60d-352d-46b8-80d7-a80143bab631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "mask_filler = pipeline(task=\"fill-mask\", model=\"./finetuned-distilbert\")\n",
    "\n",
    "def fill_in_the_blanks(sent, model, tokenizer, mask_filler):\n",
    "  # print(sent)\n",
    "  sent = sent.replace(\"[MASK]\",f\"{tokenizer.mask_token}\")\n",
    "  c = sent.count(f\"{tokenizer.mask_token}\")\n",
    "  for i in range(c):\n",
    "    s_embedding = model.encode(sent, convert_to_tensor=True)\n",
    "    sent = mask_filler(sent)\n",
    "    min_cosine_score = 1\n",
    "    for mask_candidates in sent:\n",
    "      if type(mask_candidates) is list:\n",
    "        for replacement in mask_candidates:\n",
    "          new_sent = replacement['sequence']\n",
    "          n_embedding = model.encode(new_sent, convert_to_tensor=True)\n",
    "          cosine_score = util.cos_sim(s_embedding, n_embedding)\n",
    "          if cosine_score < min_cosine_score:\n",
    "            min_cosine_score = cosine_score\n",
    "            best_candidate = new_sent\n",
    "      else:\n",
    "        new_sent = mask_candidates['sequence']\n",
    "        n_embedding = model.encode(new_sent, convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(s_embedding, n_embedding)\n",
    "        if cosine_score < min_cosine_score:\n",
    "          min_cosine_score = cosine_score\n",
    "          best_candidate = new_sent\n",
    "    sent = best_candidate\n",
    "  # print(sent)\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "767b25d9-ac3e-4fce-a915-8a94290bff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 500/500 [00:00<00:00, 331042.15it/s]\n",
      "100%|██████████████████████████████████████████████| 500/500 [06:42<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "joke_data = {'type':[], 'body':[], 'template':[]}\n",
    "\n",
    "for _ in tqdm(range(500)):\n",
    "    idx = random.randint(0, len(data))\n",
    "    sample = data[idx]\n",
    "    joke_data['type'].append('human')\n",
    "    joke_data['body'].append(sample)\n",
    "    joke_data['template'].append('N/A')\n",
    "    \n",
    "for _ in tqdm(range(500)):\n",
    "    idx = random.randint(0, len(data))\n",
    "    sample = data[idx]\n",
    "    template = get_masked_template(sample, matcher, masking_type='top_n', tau=0.4, n=0.3)\n",
    "    generated = fill_in_the_blanks(template, model, filling_tokenizer, mask_filler)\n",
    "    joke_data['type'].append('generated')\n",
    "    joke_data['body'].append(generated)\n",
    "    joke_data['template'].append(template)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd1131aa-96eb-409c-a6f7-7ad468856bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(joke_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "623b4cc2-ff8a-47ea-8fea-f8aab2be33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv('human_eval_jokes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11136b77-c09b-4cb1-a9e6-943b5e4ccf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call a tea blend that was deceptive about its ingredients? A poly tea sham.\n",
      "\n",
      "What do you call a tea blend that was deceptive about its ingredients? [MASK] [MASK] .\n",
      "\n",
      "What do you call a tea blend that was deceptive about its ingredients? [MASK] [MASK] .\n",
      "\n",
      "What do you call a tea blend that was deceptive about its ingredients? Orange cream.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What do you call a tea blend that was deceptive about its ingredients? Orange cream.'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.randint(0, len(data))\n",
    "sample = data[idx]\n",
    "print(sample)\n",
    "\n",
    "template = get_masked_template(sample, matcher, masking_type='top_n', tau=0.4, n=0.3)\n",
    "print(template)\n",
    "\n",
    "fill_in_the_blanks(template, model, filling_tokenizer, mask_filler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnlpkernel",
   "language": "python",
   "name": "advnlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
